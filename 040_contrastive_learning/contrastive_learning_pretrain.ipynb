{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (/home/mori/miniforge3/envs/torch/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Literal\n",
    "\n",
    "import deepchem as dc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Batch, Data, Dataset\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraphDataset(Dataset):\n",
    "    def __init__(self, smiles_list, transform=None):\n",
    "        super().__init__()\n",
    "        self.smiles_list = smiles_list\n",
    "        self.featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smi = self.smiles_list[idx]\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        mol = AllChem.AddHs(mol)\n",
    "        dgraph = self.featurizer.featurize([mol])[0]\n",
    "        pyg_data = self._dc_to_pyg(dgraph)\n",
    "        return pyg_data\n",
    "\n",
    "    def _dc_to_pyg(self, dgraph):\n",
    "        node_feats = torch.tensor(dgraph.node_features, dtype=torch.float)\n",
    "        edge_index = torch.tensor(dgraph.edge_index, dtype=torch.long)\n",
    "        edge_feats = torch.tensor(dgraph.edge_features, dtype=torch.float)\n",
    "\n",
    "        data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_feats)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_drop_edge(pyg_data: Data, drop_prob: float = 0.1):\n",
    "    edge_index = pyg_data.edge_index\n",
    "    edge_attr = pyg_data.edge_attr\n",
    "\n",
    "    num_edges = edge_index.size(1)\n",
    "    mask = torch.rand(num_edges) > drop_prob\n",
    "    edge_index_new = edge_index[:, mask]\n",
    "    edge_attr_new = edge_attr[mask]\n",
    "\n",
    "    aug_data = Data(\n",
    "        x=pyg_data.x.clone(), edge_index=edge_index_new, edge_attr=edge_attr_new\n",
    "    )\n",
    "    return aug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask_node(pyg_data: Data, mask_prob: float = 0.1):\n",
    "    data = Data(\n",
    "        x=pyg_data.x.clone(),\n",
    "        edge_index=pyg_data.edge_index.clone(),\n",
    "        edge_attr=pyg_data.edge_attr.clone(),\n",
    "    )\n",
    "    node_mask = torch.rand(data.x.size(0)) < mask_prob\n",
    "    data.x[node_mask] = 0.0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(\n",
    "    pyg_data: Data,\n",
    "    mode: List[Literal[\"drop_edge\", \"mask_node\"]],\n",
    "    edge_drop_prob: float = 0.1,\n",
    "    node_mask_prob: float = 0.1,\n",
    "):\n",
    "    data = pyg_data\n",
    "    if mode in [\"drop_edge\"]:\n",
    "        data = random_drop_edge(data, drop_prob=edge_drop_prob)\n",
    "    if mode in [\"mask_node\"]:\n",
    "        data = random_mask_node(data, mask_prob=node_mask_prob)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(\n",
    "    augment_mode: List[Literal[\"drop_edge\", \"mask_node\"]] = [\"drop_edge\", \"mask_node\"],\n",
    "    edge_drop_prob=0.1,\n",
    "    node_mask_prob=0.1,\n",
    "):\n",
    "    def collate_fn(batch):\n",
    "        aug1_list = []\n",
    "        aug2_list = []\n",
    "        for pyg_data in batch:\n",
    "            aug1 = augmentation(\n",
    "                pyg_data,\n",
    "                augment_mode,\n",
    "                edge_drop_prob=edge_drop_prob,\n",
    "                node_mask_prob=node_mask_prob,\n",
    "            )\n",
    "            aug2 = augmentation(\n",
    "                pyg_data,\n",
    "                augment_mode,\n",
    "                edge_drop_prob=edge_drop_prob,\n",
    "                node_mask_prob=node_mask_prob,\n",
    "            )\n",
    "            aug1_list.append(aug1)\n",
    "            aug2_list.append(aug2)\n",
    "        return aug1_list, aug2_list\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_node_features=20, num_edge_features=11, hidden_dim=64, out_dim=64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.proj_head = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h_pool = global_mean_pool(h, batch_index)\n",
    "        z = self.proj_head(h_pool)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(z1, z2, temperature=0.1):\n",
    "    z1_norm = F.normalize(z1, p=2, dim=1)\n",
    "    z2_norm = F.normalize(z2, p=2, dim=1)\n",
    "    sim_matrix = torch.matmul(z1_norm, z2_norm.t())\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    # InfoNCE ロスの計算\n",
    "    logits_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "    sim_matrix_exp = torch.exp(sim_matrix - logits_max.detach())\n",
    "    pos = torch.diag(sim_matrix_exp)\n",
    "    denom = torch.sum(sim_matrix_exp, dim=1)\n",
    "    loss = -torch.log(pos / denom)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(\n",
    "    smiles_list,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    augmentation_mode: List[Literal[\"drop_edge\", \"mask_node\"]] = [\n",
    "        \"drop_edge\",\n",
    "        \"mask_node\",\n",
    "    ],\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 2) Dataset & Dataloader 準備\n",
    "    dataset = MolGraphDataset(smiles_list)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=get_collate_fn(augment_mode=augmentation_mode),\n",
    "    )\n",
    "\n",
    "    # 3) GNNエンコーダ作成\n",
    "    encoder = GNNEncoder(num_node_features=30, num_edge_features=11)\n",
    "    encoder.to(device)  # GPU/CPUに転送\n",
    "\n",
    "    # 4) Optimizer\n",
    "    optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "\n",
    "    # 5) 学習ループ\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        encoder.train()\n",
    "        total_loss = 0.0\n",
    "        for i, (aug1_list, aug2_list) in enumerate(dataloader):\n",
    "            batch_aug1 = Batch.from_data_list(aug1_list).to(device)\n",
    "            batch_aug2 = Batch.from_data_list(aug2_list).to(device)\n",
    "            # 順伝搬\n",
    "            z1 = encoder(\n",
    "                batch_aug1.x,\n",
    "                batch_aug1.edge_index,\n",
    "                batch_aug1.edge_attr,\n",
    "                batch_aug1.batch,\n",
    "            )\n",
    "            z2 = encoder(\n",
    "                batch_aug2.x,\n",
    "                batch_aug2.edge_index,\n",
    "                batch_aug2.edge_attr,\n",
    "                batch_aug2.batch,\n",
    "            )\n",
    "\n",
    "            # InfoNCE loss 計算\n",
    "            loss = info_nce_loss(z1, z2, temperature=0.1)\n",
    "\n",
    "            # 逆伝搬 & パラメータ更新\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{epochs}] | Batch [{i}/{len(dataloader)}] | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{epochs}] | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(encoder.state_dict(), \"pretrained_gnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-22 21:04:28--  https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/pubchem_10m.txt.zip\n",
      "Resolving deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)... 3.5.160.186, 3.5.160.162, 52.219.220.178, ...\n",
      "Connecting to deepchemdata.s3-us-west-1.amazonaws.com (deepchemdata.s3-us-west-1.amazonaws.com)|3.5.160.186|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 130376753 (124M) [application/zip]\n",
      "Saving to: ‘pubchem_10m.txt.zip’\n",
      "\n",
      "pubchem_10m.txt.zip 100%[===================>] 124.34M  8.93MB/s    in 15s     \n",
      "\n",
      "2024-12-22 21:04:44 (8.07 MB/s) - ‘pubchem_10m.txt.zip’ saved [130376753/130376753]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/pubchem_10m.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "\n",
    "zip_file_path = \"pubchem_10m.txt.zip\"\n",
    "lines_to_read = 1000\n",
    "smiles_list = []\n",
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_file:\n",
    "    file_names = zip_file.namelist()\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with zip_file.open(file_name) as file:\n",
    "                with io.TextIOWrapper(file, encoding=\"utf-8\") as text_file:\n",
    "                    for i, line in enumerate(text_file):\n",
    "                        if i >= lines_to_read:\n",
    "                            break\n",
    "                        smiles_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/10] | Loss: 0.2442\n",
      "Epoch [2/10] | Loss: 0.0461\n",
      "Epoch [3/10] | Loss: 0.0329\n",
      "Epoch [4/10] | Loss: 0.0327\n",
      "Epoch [5/10] | Loss: 0.0288\n",
      "Epoch [6/10] | Loss: 0.0230\n",
      "Epoch [7/10] | Loss: 0.0269\n",
      "Epoch [8/10] | Loss: 0.0256\n",
      "Epoch [9/10] | Loss: 0.0154\n",
      "Epoch [10/10] | Loss: 0.0147\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_encoder = train_contrastive(\n",
    "    smiles_list=smiles_list,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    lr=1e-3,\n",
    "    augmentation_mode=[\"drop_edge\", \"mask_node\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1437/250564323.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_encoder.load_state_dict(torch.load(\"pretrained_gnn.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_encoder = GNNEncoder(num_node_features=30, num_edge_features=11)\n",
    "pretrained_encoder.load_state_dict(torch.load(\"pretrained_gnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
