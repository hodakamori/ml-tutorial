{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oQsY_9ip2OyV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRAを使わない2層MLP。\n",
        "    - Linear -> ReLU -> Linear\n",
        "    - hidden_dimを大きくしてパラメータを増やしている。\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=768, hidden_dim=512, output_dim=10):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2sTT7Uw020HV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRALinear(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRA (Low-Rank Adaptation) を適用した線形層のサンプル実装。\n",
        "    - 元の重み W は凍結し更新しない\n",
        "    - 代わりに低ランク行列 A, B のみ学習し、W + (alpha/r) * (B @ A) の形で線形変換を行う\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, r=4, alpha=1.0, bias=True):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # 元の重み (freeze)\n",
        "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        self.weight.requires_grad = False  # ← 凍結して学習しない\n",
        "\n",
        "        # バイアス（必要なら学習OK）\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # LoRA用 低ランク行列\n",
        "        self.lora_A = nn.Parameter(torch.empty(r, in_features))\n",
        "        self.lora_B = nn.Parameter(torch.empty(out_features, r))\n",
        "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_B)\n",
        "\n",
        "        # スケーリング係数\n",
        "        self.scaling = alpha / r\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        y = x * [W + (alpha/r)*(B@A)]^T + bias\n",
        "        \"\"\"\n",
        "        # LoRA のアップデート項 (B @ A)\n",
        "        lora_update = self.lora_B @ self.lora_A\n",
        "        # 最終的な重み\n",
        "        effective_weight = self.weight + self.scaling * lora_update\n",
        "        return F.linear(x, effective_weight, self.bias)"
      ],
      "metadata": {
        "id": "EvPGviOd2PPX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAModel(nn.Module):\n",
        "    \"\"\"\n",
        "    中間層に LoRALinear を使った2層MLP。\n",
        "    - 最初の層をLoRALinearにして、重みは凍結 + 低ランク行列のみ学習\n",
        "    - 2層目は普通の nn.Linear (任意でLoRAにしてもOK)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=768, hidden_dim=512, output_dim=10,\n",
        "                 lora_r=4, lora_alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.lora_linear = LoRALinear(input_dim, hidden_dim,\n",
        "                                      r=lora_r, alpha=lora_alpha)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.lora_linear(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2mktODzn2Too"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# 大きめの次元数を例として設定\n",
        "input_dim = 768\n",
        "hidden_dim = 512\n",
        "output_dim = 10\n",
        "batch_size = 32\n",
        "num_epochs = 3  # 簡単に回す\n",
        "\n",
        "# (1) ベースラインモデル (LoRAなし)\n",
        "baseline_model = BaselineModel(\n",
        "    input_dim,\n",
        "    hidden_dim,\n",
        "    output_dim\n",
        "  )\n",
        "# (2) LoRA付きモデル\n",
        "#    低ランク次元 r を小さくすると追加パラメータが少ない\n",
        "lora_r = 4\n",
        "lora_alpha = 8\n",
        "lora_model = LoRAModel(\n",
        "    input_dim,\n",
        "    hidden_dim,\n",
        "    output_dim,\n",
        "    lora_r,\n",
        "    lora_alpha\n",
        "  )"
      ],
      "metadata": {
        "id": "Tcv8btOJ2dvg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "# パラメータ数を表示\n",
        "base_total, base_train = count_params(baseline_model)\n",
        "lora_total, lora_train = count_params(lora_model)\n",
        "\n",
        "print(\"=== Baseline Model ===\")\n",
        "print(f\"Total params    : {base_total:,}\")\n",
        "print(f\"Trainable params: {base_train:,}\")\n",
        "\n",
        "print(\"\\n=== LoRA Model ===\")\n",
        "print(f\"Total params    : {lora_total:,}\")\n",
        "print(f\"Trainable params: {lora_train:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gppq_2_B4GYW",
        "outputId": "e73fedd7-3121-4b12-dfc8-f0a3153da488"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Baseline Model ===\n",
            "Total params    : 398,858\n",
            "Trainable params: 398,858\n",
            "\n",
            "=== LoRA Model ===\n",
            "Total params    : 403,978\n",
            "Trainable params: 10,762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_dummy = torch.randn(batch_size, input_dim)\n",
        "y_dummy = torch.randint(0, output_dim, (batch_size,))\n",
        "\n",
        "# オプティマイザ\n",
        "baseline_opt = optim.SGD(baseline_model.parameters(), lr=0.01)\n",
        "lora_opt = optim.SGD(lora_model.parameters(), lr=0.01)\n",
        "\n",
        "print(\"\\n==== 学習開始 ====\")\n",
        "for epoch in range(num_epochs):\n",
        "    # (A) Baseline\n",
        "    baseline_opt.zero_grad()\n",
        "    logits_base = baseline_model(X_dummy)\n",
        "    loss_base = F.cross_entropy(logits_base, y_dummy)\n",
        "    loss_base.backward()\n",
        "    baseline_opt.step()\n",
        "\n",
        "    # (B) LoRA\n",
        "    lora_opt.zero_grad()\n",
        "    logits_lora = lora_model(X_dummy)\n",
        "    loss_lora = F.cross_entropy(logits_lora, y_dummy)\n",
        "    loss_lora.backward()\n",
        "    lora_opt.step()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
        "          f\"BaselineLoss={loss_base.item():.4f}, \"\n",
        "          f\"LoRALoss={loss_lora.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFT_kXSL2ntm",
        "outputId": "9ecc4068-44d3-4e7a-abf7-4a31c079e450"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 学習開始 ====\n",
            "[Epoch 1/3] BaselineLoss=2.3053, LoRALoss=2.3254\n",
            "[Epoch 2/3] BaselineLoss=2.2467, LoRALoss=2.3022\n",
            "[Epoch 3/3] BaselineLoss=2.1896, LoRALoss=2.2794\n"
          ]
        }
      ]
    }
  ]
}